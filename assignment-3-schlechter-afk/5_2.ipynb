{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "data = np.load(\"permuted_mnist.npz\")\n",
    "permuted_x_train = data[\"train_images\"]\n",
    "y_train = data[\"train_labels\"]\n",
    "permuted_x_test = data[\"test_images\"]\n",
    "y_test = data[\"test_labels\"]\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(permuted_x_train), torch.from_numpy(y_train))\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.from_numpy(permuted_x_test), torch.from_numpy(y_test))\n",
    "\n",
    "# train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "# test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.2, random_state=seed)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes):\n",
    "        super(MLPModel, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Flatten())\n",
    "        for i in range(len(hidden_sizes)):\n",
    "            layers.append(nn.Linear(input_size if i == 0 else hidden_sizes[i - 1], hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], num_classes))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "input_size = 28 * 28  # Input size is the flattened image\n",
    "hidden_sizes = [128, 64]  # Example hidden layer sizes\n",
    "output_size = 10  # Number of classes (digits)\n",
    "\n",
    "model = MLPModel(input_size, hidden_sizes, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        images = images.view(images.size(0), -1)\n",
    "        # Convert the images tensor from Byte to Float\n",
    "        images = images.float()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.99\n",
      "Validation Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.view(images.size(0), -1)\n",
    "            images = images.float()\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "train_accuracy = evaluate(model, train_loader)\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "\n",
    "val_accuracy = evaluate(model, val_loader)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers_list = [1, 2, 3]  # Vary the number of hidden layers\n",
    "neurons_list = [64, 128, 256]  # Vary the number of neurons in each layer\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "for num_hidden_layers in hidden_layers_list:\n",
    "    for num_neurons in neurons_list:\n",
    "        # Create the model with the current hyperparameters\n",
    "        hidden_sizes = [num_neurons] * num_hidden_layers\n",
    "        model = MLPModel(input_size, hidden_sizes, output_size)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for images, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                images = images.view(images.size(0), -1)\n",
    "                images = images.float()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluate on the validation set\n",
    "        val_accuracy = evaluate(model, val_loader)\n",
    "\n",
    "        # Check if the current model is the best\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: MLPModel(\n",
      "  (model): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Best Hyperparameters - Hidden Layers: 3, Neurons: 256\n",
      "Test Accuracy of the Best Model: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Report the best model's hyperparameters\n",
    "print(f\"Best Model: {best_model}\")\n",
    "print(f\"Best Hyperparameters - Hidden Layers: {num_hidden_layers}, Neurons: {num_neurons}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_accuracy = evaluate(best_model, test_loader)\n",
    "print(f\"Test Accuracy of the Best Model: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "test_accuracy = evaluate(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN ON PERMUTED_MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 98.66%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def setKernelsize(self, kernelsize):\n",
    "        self.kernelsize = kernelsize\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=kernelsize, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=kernelsize, stride=1, padding=1)\n",
    "\n",
    "    def setStride(self, stride):\n",
    "        self.stride = stride\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=self.kernelsize, stride=stride, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=self.kernelsize, stride=stride, padding=1)\n",
    "\n",
    "    def setDropout(self, dropout):\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "Model = CNN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    Model.train()\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        images = images.float()\n",
    "        outputs = Model(images.unsqueeze(1))  # Add channel dimension\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "Model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in train_loader:\n",
    "        images = images.float()\n",
    "        outputs = Model(images.unsqueeze(1))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "train_accuracy = 100 * correct / total\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 94.70\n",
      "Test Accuracy: 95.13\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.float()\n",
    "            outputs = model(images.unsqueeze(1))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    accuracy *= 100.0\n",
    "    return accuracy\n",
    "\n",
    "val_accuracy = evaluate(Model, val_loader)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.2f}\")\n",
    "\n",
    "test_accuracy = evaluate(Model, test_loader)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:  94.27499999999999\n",
      "Validation Accuracy:  95.6\n",
      "Validation Accuracy:  95.15833333333333\n"
     ]
    }
   ],
   "source": [
    "# perform hyperparameter tuning by varying the kernel size, dropout rate and learning rate in a few tuples\n",
    "# kernel size, dropout rate, learning rate\n",
    "\n",
    "hyperparameter_tuples = [(3, 0.3, 0.001), (3, 0.5, 0.0005), (3, 0.7, 0.001), (5, 0.3, 0.001), (7, 0.5, 0.0005)]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "for kernelsize, dropout, learning_rate in hyperparameter_tuples:\n",
    "    Model = CNN()\n",
    "    Model.setKernelsize(kernelsize)\n",
    "    Model.setDropout(dropout)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(Model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        Model.train()\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            images = images.float()\n",
    "            outputs = Model(images.unsqueeze(1))\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    val_accuracy = evaluate(Model, val_loader)\n",
    "    print(\"Validation Accuracy: \", val_accuracy)\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        best_model = Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy (validation dataset):  95.6\n",
      "Best Model:  CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Accuracy (validation dataset): \", best_accuracy)\n",
    "print(\"Best Model: \", best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis:\n",
    "\n",
    "Detailed analysis in Part-5 Report.\n",
    "\n",
    "1. Observed Differences and Challenges:\n",
    "\n",
    "Training and Evaluation Times: CNNs are generally more computationally intensive during training due to their convolutional layers. This results in longer training times compared to MLPs.\n",
    "\n",
    "Data Augmentation: For both datasets, data augmentation techniques like rotation, scaling, and translation can significantly benefit CNNs by increasing the variety of learned features. MLPs may not benefit as much from data augmentation.\n",
    "\n",
    "Overfitting: CNNs have more parameters and are prone to overfitting when there's limited data. Overfitting can be mitigated with techniques like dropout and weight decay. MLPs, having fewer parameters, might be less prone to overfitting.\n",
    "\n",
    "1. Potential for Overfitting and Continual Learning:\n",
    "\n",
    "Overfitting in CNN: CNNs have a higher potential for overfitting, especially on smaller datasets, due to their larger number of learnable parameters. Regularization techniques like dropout and weight decay are essential to prevent overfitting.\n",
    "\n",
    "Overfitting in MLP: While MLPs are less prone to overfitting due to fewer parameters, they can still overfit, particularly when the model is too complex or the dataset is small. Regularization is also useful for MLPs.\n",
    "\n",
    "Continual Learning: Continual learning, or lifelong learning, is the ability of a model to learn from a sequence of tasks without forgetting what it learned previously. Both CNNs and MLPs can suffer from catastrophic forgetting when not specifically designed for continual learning. Techniques like Elastic Weight Consolidation (EWC) and Progressive Neural Networks (PNN) can be applied to mitigate this issue.\n",
    "\n",
    "In summary, CNNs outperform MLPs on image datasets like MN2IST due to their ability to capture spatial features efficiently. However, for the Permuted MNIST dataset, both models face challenges in recognizing digits without spatial information. Continual learning techniques are crucial when dealing with a sequence of tasks, ensuring that previously learned knowledge is retained while adapting to new tasks. Proper regularization and data augmentation are key factors in preventing overfitting in both CNNs and MLPs. The choice of model depends on the nature of the dataset and the specific task requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
