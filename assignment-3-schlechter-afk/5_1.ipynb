{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:7sp0qc9s) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁█</td></tr><tr><td>val_accuracy1</td><td>▁▁</td></tr><tr><td>val_accuracy2</td><td>▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>val_accuracy1</td><td>0.0</td></tr><tr><td>val_accuracy2</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sleek-dew-3</strong> at: <a href='https://wandb.ai/swayamagrawal1004/Part_5_1/runs/7sp0qc9s' target=\"_blank\">https://wandb.ai/swayamagrawal1004/Part_5_1/runs/7sp0qc9s</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_121326-7sp0qc9s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:7sp0qc9s). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/swayam/github-classroom/Statistical-Methods-in-AI-Monsoon-2023/assignment-3-schlechter-afk/wandb/run-20231026_121508-qrn1c80e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/swayamagrawal1004/Part_5_1/runs/qrn1c80e' target=\"_blank\">unique-energy-4</a></strong> to <a href='https://wandb.ai/swayamagrawal1004/Part_5_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/swayamagrawal1004/Part_5_1' target=\"_blank\">https://wandb.ai/swayamagrawal1004/Part_5_1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/swayamagrawal1004/Part_5_1/runs/qrn1c80e' target=\"_blank\">https://wandb.ai/swayamagrawal1004/Part_5_1/runs/qrn1c80e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/swayamagrawal1004/Part_5_1/runs/qrn1c80e?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x179242100>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key='fac5cd7f576f5d6d2591b0e77385c09a7922b210')\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Part_5_1\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.04,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 58000\n",
      "Number of validation samples: 14000\n",
      "Number of testing samples: 18000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class CustomMultiMNISTDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for folder in os.listdir(root):\n",
    "            if os.path.isdir(os.path.join(root, folder)):\n",
    "                digit1 = int(folder[0])\n",
    "                digit2 = int(folder[1])\n",
    "                if digit1 != digit2:\n",
    "                    for filename in os.listdir(os.path.join(root, folder)):\n",
    "                        self.image_paths.append(os.path.join(root, folder, filename))\n",
    "                        self.labels.append((digit1, digit2))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        label = self.labels[index]\n",
    "        # print(image_path, label)\n",
    "        with open(image_path, 'rb') as f:\n",
    "            image = Image.open(f)\n",
    "            if self.transform is not None:\n",
    "                image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  \n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = CustomMultiMNISTDataset(\"../double_mnist_seed_123_image_size_64_64/train\", transform=transform)\n",
    "val_dataset = CustomMultiMNISTDataset(\"../double_mnist_seed_123_image_size_64_64/val\", transform=transform)\n",
    "test_dataset = CustomMultiMNISTDataset(\"../double_mnist_seed_123_image_size_64_64/test\", transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of validation samples:\", len(val_dataset))\n",
    "print(\"Number of testing samples:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes):\n",
    "        super(MLPModel, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Flatten())\n",
    "        for i in range(len(hidden_sizes)):\n",
    "            layers.append(nn.Linear(input_size if i == 0 else hidden_sizes[i - 1], hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], num_classes))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Validation Accuracy (Digit 1): 37.59%\n",
      "Epoch 1/10, Validation Accuracy (Digit 2): 45.85%\n",
      "Epoch 2/10, Validation Accuracy (Digit 1): 42.59%\n",
      "Epoch 2/10, Validation Accuracy (Digit 2): 52.79%\n",
      "Epoch 3/10, Validation Accuracy (Digit 1): 45.76%\n",
      "Epoch 3/10, Validation Accuracy (Digit 2): 54.92%\n",
      "Epoch 4/10, Validation Accuracy (Digit 1): 44.96%\n",
      "Epoch 4/10, Validation Accuracy (Digit 2): 56.32%\n",
      "Epoch 5/10, Validation Accuracy (Digit 1): 46.44%\n",
      "Epoch 5/10, Validation Accuracy (Digit 2): 54.45%\n",
      "Epoch 6/10, Validation Accuracy (Digit 1): 49.49%\n",
      "Epoch 6/10, Validation Accuracy (Digit 2): 50.96%\n",
      "Epoch 7/10, Validation Accuracy (Digit 1): 48.75%\n",
      "Epoch 7/10, Validation Accuracy (Digit 2): 54.36%\n",
      "Epoch 8/10, Validation Accuracy (Digit 1): 51.11%\n",
      "Epoch 8/10, Validation Accuracy (Digit 2): 53.13%\n",
      "Epoch 9/10, Validation Accuracy (Digit 1): 48.75%\n",
      "Epoch 9/10, Validation Accuracy (Digit 2): 54.43%\n",
      "Epoch 10/10, Validation Accuracy (Digit 1): 49.54%\n",
      "Epoch 10/10, Validation Accuracy (Digit 2): 54.39%\n",
      "Test Accuracy (Digit 1): 52.04%\n",
      "Test Accuracy (Digit 2): 51.96%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "input_size = 64 * 64  \n",
    "hidden_sizes = [128, 64]  \n",
    "num_classes_per_digit = 10  \n",
    "total_num_classes = 2 * num_classes_per_digit\n",
    "\n",
    "model = MLPModel(input_size, [256, 128], total_num_classes)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        label1 = torch.zeros(len(labels[0]), num_classes_per_digit)\n",
    "        label2 = torch.zeros(len(labels[1]), num_classes_per_digit)\n",
    "        for i in range(len(labels[0])):\n",
    "            label1[i, labels[0][i]] = 1\n",
    "        for i in range(len(labels[1])):\n",
    "            label2[i, labels[1][i]] = 1\n",
    "        labels = torch.cat((label1, label2), dim=1)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct1 = 0\n",
    "        correct2 = 0\n",
    "        total = 0\n",
    "        for images, labels in val_loader:\n",
    "            label1 = torch.zeros(len(labels[0]), num_classes_per_digit)\n",
    "            label2 = torch.zeros(len(labels[1]), num_classes_per_digit)\n",
    "            for i in range(len(labels[0])):\n",
    "                label1[i, labels[0][i]] = 1\n",
    "            for i in range(len(labels[1])):\n",
    "                label2[i, labels[1][i]] = 1\n",
    "            labels = torch.cat((label1, label2), dim=1)\n",
    "            outputs = model(images)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            val1 =  (predicted[:, :num_classes_per_digit] == label1).all(1).sum().item()\n",
    "            correct1 += val1\n",
    "            val2 = (predicted[:, num_classes_per_digit:] == label2).all(1).sum().item()\n",
    "            correct2 += val2\n",
    "\n",
    "        accuracy1 = 100 * correct1 / total\n",
    "        accuracy2 = 100 * correct2 / total\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Validation Accuracy (Digit 1): {accuracy1:.2f}%')\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Validation Accuracy (Digit 2): {accuracy2:.2f}%')\n",
    "        # do wandb log of validation accuracy and epochs for both digits 1 and 2:\n",
    "        wandb.log({\"val_accuracy1\": accuracy1, \"val_accuracy2\": accuracy2, \"epoch\": epoch})\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "## Evaluate on test set\n",
    "with torch.no_grad():\n",
    "    correct1 = 0\n",
    "    correct2 = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        label1 = torch.zeros(len(labels[0]), num_classes_per_digit)\n",
    "        label2 = torch.zeros(len(labels[1]), num_classes_per_digit)\n",
    "        for i in range(len(labels[0])):\n",
    "            label1[i, labels[0][i]] = 1\n",
    "        for i in range(len(labels[1])):\n",
    "            label2[i, labels[1][i]] = 1\n",
    "        labels = torch.cat((label1, label2), dim=1)\n",
    "        outputs = model(images)\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct1 += (predicted[:, :num_classes_per_digit] == label1).all(1).sum().item()\n",
    "        correct2 += (predicted[:, num_classes_per_digit:] == label2).all(1).sum().item()\n",
    "\n",
    "    test_accuracy1 = 100 * correct1 / total\n",
    "    test_accuracy2 = 100 * correct2 / total\n",
    "\n",
    "print(f'Test Accuracy (Digit 1): {test_accuracy1:.2f}%')\n",
    "print(f'Test Accuracy (Digit 2): {test_accuracy2:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29000 7000 1800\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import copy\n",
    "\n",
    "train_data_hyper = copy.deepcopy(train_dataset)\n",
    "valid_data_hyper = copy.deepcopy(val_dataset)\n",
    "test_data_hyper = copy.deepcopy(test_dataset)\n",
    "\n",
    "df = pd.DataFrame({'image_path': train_dataset.image_paths, 'label': train_dataset.labels})\n",
    "df = shuffle(df)\n",
    "\n",
    "df = df.sample(frac=0.5)\n",
    "\n",
    "train_data_hyper.image_paths = df.image_path.to_list()\n",
    "train_data_hyper.labels = df.label.to_list()\n",
    "\n",
    "df = pd.DataFrame({'image_path': val_dataset.image_paths, 'label': val_dataset.labels})\n",
    "df = shuffle(df)\n",
    "df = df.sample(frac=0.5)\n",
    "valid_data_hyper.image_paths = df.image_path.to_list()\n",
    "valid_data_hyper.labels = df.label.to_list()\n",
    "\n",
    "df = pd.DataFrame({'image_path': test_dataset.image_paths, 'label': test_dataset.labels})\n",
    "df = shuffle(df)\n",
    "df = df.sample(frac=0.1)\n",
    "\n",
    "test_data_hyper.image_paths = df.image_path.to_list()\n",
    "test_data_hyper.labels = df.label.to_list()\n",
    "\n",
    "print(len(train_data_hyper), len(valid_data_hyper), len(test_data_hyper))\n",
    "\n",
    "batch_size = 64\n",
    "train_loader_hyper = DataLoader(train_data_hyper, batch_size=batch_size, shuffle=True)\n",
    "val_loader_hyper = DataLoader(valid_data_hyper, batch_size=batch_size)\n",
    "test_loader_hyper = DataLoader(test_data_hyper, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Validation Accuracy (Digit 1): 16.49%\n",
      "Epoch 1/10, Validation Accuracy (Digit 2): 25.93%\n",
      "Epoch 2/10, Validation Accuracy (Digit 1): 32.31%\n",
      "Epoch 2/10, Validation Accuracy (Digit 2): 42.93%\n",
      "Epoch 3/10, Validation Accuracy (Digit 1): 42.96%\n",
      "Epoch 3/10, Validation Accuracy (Digit 2): 47.73%\n",
      "Epoch 4/10, Validation Accuracy (Digit 1): 46.17%\n",
      "Epoch 4/10, Validation Accuracy (Digit 2): 49.56%\n",
      "Epoch 5/10, Validation Accuracy (Digit 1): 44.54%\n",
      "Epoch 5/10, Validation Accuracy (Digit 2): 54.69%\n",
      "Epoch 6/10, Validation Accuracy (Digit 1): 49.21%\n",
      "Epoch 6/10, Validation Accuracy (Digit 2): 51.73%\n",
      "Epoch 7/10, Validation Accuracy (Digit 1): 46.20%\n",
      "Epoch 7/10, Validation Accuracy (Digit 2): 54.34%\n",
      "Epoch 8/10, Validation Accuracy (Digit 1): 49.67%\n",
      "Epoch 8/10, Validation Accuracy (Digit 2): 52.56%\n",
      "Epoch 9/10, Validation Accuracy (Digit 1): 50.14%\n",
      "Epoch 9/10, Validation Accuracy (Digit 2): 53.79%\n",
      "Epoch 10/10, Validation Accuracy (Digit 1): 50.51%\n",
      "Epoch 10/10, Validation Accuracy (Digit 2): 52.10%\n",
      "Epoch 1/10, Validation Accuracy (Digit 1): 23.26%\n",
      "Epoch 1/10, Validation Accuracy (Digit 2): 31.99%\n",
      "Epoch 2/10, Validation Accuracy (Digit 1): 35.13%\n",
      "Epoch 2/10, Validation Accuracy (Digit 2): 43.94%\n",
      "Epoch 3/10, Validation Accuracy (Digit 1): 38.67%\n",
      "Epoch 3/10, Validation Accuracy (Digit 2): 48.20%\n",
      "Epoch 4/10, Validation Accuracy (Digit 1): 43.89%\n",
      "Epoch 4/10, Validation Accuracy (Digit 2): 48.90%\n",
      "Epoch 5/10, Validation Accuracy (Digit 1): 43.21%\n",
      "Epoch 5/10, Validation Accuracy (Digit 2): 49.71%\n",
      "Epoch 6/10, Validation Accuracy (Digit 1): 43.57%\n",
      "Epoch 6/10, Validation Accuracy (Digit 2): 50.80%\n",
      "Epoch 7/10, Validation Accuracy (Digit 1): 41.03%\n",
      "Epoch 7/10, Validation Accuracy (Digit 2): 52.67%\n",
      "Epoch 8/10, Validation Accuracy (Digit 1): 47.49%\n",
      "Epoch 8/10, Validation Accuracy (Digit 2): 48.60%\n",
      "Epoch 9/10, Validation Accuracy (Digit 1): 46.01%\n",
      "Epoch 9/10, Validation Accuracy (Digit 2): 52.47%\n",
      "Epoch 10/10, Validation Accuracy (Digit 1): 45.70%\n",
      "Epoch 10/10, Validation Accuracy (Digit 2): 48.07%\n",
      "Epoch 1/10, Validation Accuracy (Digit 1): 14.31%\n",
      "Epoch 1/10, Validation Accuracy (Digit 2): 22.43%\n",
      "Epoch 2/10, Validation Accuracy (Digit 1): 24.93%\n",
      "Epoch 2/10, Validation Accuracy (Digit 2): 32.49%\n",
      "Epoch 3/10, Validation Accuracy (Digit 1): 36.43%\n",
      "Epoch 3/10, Validation Accuracy (Digit 2): 39.30%\n",
      "Epoch 4/10, Validation Accuracy (Digit 1): 39.07%\n",
      "Epoch 4/10, Validation Accuracy (Digit 2): 44.71%\n",
      "Epoch 5/10, Validation Accuracy (Digit 1): 42.03%\n",
      "Epoch 5/10, Validation Accuracy (Digit 2): 44.93%\n",
      "Epoch 6/10, Validation Accuracy (Digit 1): 42.69%\n",
      "Epoch 6/10, Validation Accuracy (Digit 2): 47.17%\n",
      "Epoch 7/10, Validation Accuracy (Digit 1): 43.80%\n",
      "Epoch 7/10, Validation Accuracy (Digit 2): 50.89%\n",
      "Epoch 8/10, Validation Accuracy (Digit 1): 44.10%\n",
      "Epoch 8/10, Validation Accuracy (Digit 2): 49.34%\n",
      "Epoch 9/10, Validation Accuracy (Digit 1): 44.13%\n",
      "Epoch 9/10, Validation Accuracy (Digit 2): 48.83%\n",
      "Epoch 10/10, Validation Accuracy (Digit 1): 45.83%\n",
      "Epoch 10/10, Validation Accuracy (Digit 2): 48.97%\n",
      "Epoch 1/10, Validation Accuracy (Digit 1): 15.54%\n",
      "Epoch 1/10, Validation Accuracy (Digit 2): 26.86%\n",
      "Epoch 2/10, Validation Accuracy (Digit 1): 31.20%\n",
      "Epoch 2/10, Validation Accuracy (Digit 2): 36.87%\n",
      "Epoch 3/10, Validation Accuracy (Digit 1): 32.37%\n",
      "Epoch 3/10, Validation Accuracy (Digit 2): 37.97%\n",
      "Epoch 4/10, Validation Accuracy (Digit 1): 36.79%\n",
      "Epoch 4/10, Validation Accuracy (Digit 2): 44.13%\n",
      "Epoch 5/10, Validation Accuracy (Digit 1): 38.40%\n",
      "Epoch 5/10, Validation Accuracy (Digit 2): 45.93%\n",
      "Epoch 6/10, Validation Accuracy (Digit 1): 41.91%\n",
      "Epoch 6/10, Validation Accuracy (Digit 2): 47.56%\n",
      "Epoch 7/10, Validation Accuracy (Digit 1): 42.24%\n",
      "Epoch 7/10, Validation Accuracy (Digit 2): 45.60%\n",
      "Epoch 8/10, Validation Accuracy (Digit 1): 45.20%\n",
      "Epoch 8/10, Validation Accuracy (Digit 2): 44.73%\n",
      "Epoch 9/10, Validation Accuracy (Digit 1): 43.27%\n",
      "Epoch 9/10, Validation Accuracy (Digit 2): 45.79%\n",
      "Epoch 10/10, Validation Accuracy (Digit 1): 44.59%\n",
      "Epoch 10/10, Validation Accuracy (Digit 2): 48.80%\n",
      "[([256, 256], 0.0025, 50.51428571428571, 52.1, 51.0), ([256, 256], 0.005, 45.7, 48.07142857142857, 46.0), ([128, 128], 0.0025, 45.82857142857143, 48.97142857142857, 47.0), ([128, 128], 0.005, 44.58571428571429, 48.8, 46.0)]\n"
     ]
    }
   ],
   "source": [
    "## Hyperparameter tuning with 4 tuples on validation dataset\n",
    "\n",
    "hidden_layers = [[256, 256], [128, 128]]\n",
    "learning_rates = [0.0025, 0.005]\n",
    "num_epochs = 10\n",
    "results = []\n",
    "\n",
    "for hidden_layer in hidden_layers:\n",
    "    for lr in learning_rates:\n",
    "\n",
    "        model = MLPModel(input_size, hidden_layer, total_num_classes)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for images, labels in train_loader_hyper:\n",
    "                label1 = torch.zeros(len(labels[0]), num_classes_per_digit)\n",
    "                label2 = torch.zeros(len(labels[1]), num_classes_per_digit)\n",
    "                for i in range(len(labels[0])):\n",
    "                    label1[i, labels[0][i]] = 1\n",
    "                for i in range(len(labels[1])):\n",
    "                    label2[i, labels[1][i]] = 1\n",
    "                labels = torch.cat((label1, label2), dim=1)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                correct1 = 0\n",
    "                correct2 = 0\n",
    "                total = 0\n",
    "                for images, labels in val_loader_hyper:\n",
    "                    label1 = torch.zeros(len(labels[0]), num_classes_per_digit)\n",
    "                    label2 = torch.zeros(len(labels[1]), num_classes_per_digit)\n",
    "                    for i in range(len(labels[0])):\n",
    "                        label1[i, labels[0][i]] = 1\n",
    "                    for i in range(len(labels[1])):\n",
    "                        label2[i, labels[1][i]] = 1\n",
    "                    labels = torch.cat((label1, label2), dim=1)\n",
    "                    outputs = model(images)\n",
    "                    predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                    total += labels.size(0)\n",
    "                    val1 =  (predicted[:, :num_classes_per_digit] == label1).all(1).sum().item()\n",
    "                    correct1 += val1\n",
    "                    val2 = (predicted[:, num_classes_per_digit:] == label2).all(1).sum().item()\n",
    "                    correct2 += val2\n",
    "\n",
    "                accuracy1 = 100 * correct1 / total\n",
    "                accuracy2 = 100 * correct2 / total\n",
    "                aggregate_accuracy = (accuracy1 + accuracy2) // 2\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Validation Accuracy (Digit 1): {accuracy1:.2f}%')\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Validation Accuracy (Digit 2): {accuracy2:.2f}%')\n",
    "\n",
    "                if epoch == num_epochs - 1:\n",
    "                    results.append((hidden_layer, lr, accuracy1, accuracy2, aggregate_accuracy))\n",
    "                    wandb.log({\"hidden_layer\": hidden_layer, \"lr\": lr, \"accuracy1\": accuracy1, \"accuracy2\": accuracy2, \"aggregate_accuracy\": aggregate_accuracy})\n",
    "\n",
    "            \n",
    "        model.eval()\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy (digit 1): 53.27777777777778\n",
      "Test accuracy (digit 2): 53.611111111111114\n"
     ]
    }
   ],
   "source": [
    "# sort results on basis of aggregate accuracy\n",
    "results.sort(key=lambda x: x[4], reverse=True)\n",
    "\n",
    "optimal_hidden_layer = results[0][0]\n",
    "optimal_lr = results[0][1]\n",
    "\n",
    "# evaluate test data on best model\n",
    "model = MLPModel(input_size, optimal_hidden_layer, total_num_classes)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=optimal_lr)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        label1 = torch.zeros(len(labels[0]), num_classes_per_digit)\n",
    "        label2 = torch.zeros(len(labels[1]), num_classes_per_digit)\n",
    "        for i in range(len(labels[0])):\n",
    "            label1[i, labels[0][i]] = 1\n",
    "        for i in range(len(labels[1])):\n",
    "            label2[i, labels[1][i]] = 1\n",
    "        labels = torch.cat((label1, label2), dim=1)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct1 = 0\n",
    "    correct2 = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader_hyper:\n",
    "        label1 = torch.zeros(len(labels[0]), num_classes_per_digit)\n",
    "        label2 = torch.zeros(len(labels[1]), num_classes_per_digit)\n",
    "        for i in range(len(labels[0])):\n",
    "            label1[i, labels[0][i]] = 1\n",
    "        for i in range(len(labels[1])):\n",
    "            label2[i, labels[1][i]] = 1\n",
    "        labels = torch.cat((label1, label2), dim=1)\n",
    "        outputs = model(images)\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct1 += (predicted[:, :num_classes_per_digit] == label1).all(1).sum().item()\n",
    "        correct2 += (predicted[:, num_classes_per_digit:] == label2).all(1).sum().item()\n",
    "    test_accuracy1 = 100 * correct1 / total\n",
    "    test_accuracy2 = 100 * correct2 / total\n",
    "\n",
    "print(\"Test accuracy (digit 1):\", test_accuracy1)\n",
    "print(\"Test accuracy (digit 2):\", test_accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 58000\n",
      "Number of validation samples: 14000\n",
      "Number of testing samples: 18000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomMultiMNISTDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for folder in os.listdir(root):\n",
    "            if os.path.isdir(os.path.join(root, folder)):\n",
    "                digit1 = int(folder[0])\n",
    "                digit2 = int(folder[1])\n",
    "                if digit1 != digit2:\n",
    "                    for filename in os.listdir(os.path.join(root, folder)):\n",
    "                        self.image_paths.append(os.path.join(root, folder, filename))\n",
    "                        self.labels.append((digit1, digit2))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        label = self.labels[index]\n",
    "        with open(image_path, 'rb') as f:\n",
    "            image = Image.open(f)\n",
    "            if self.transform is not None:\n",
    "                image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def setKernelsize(self, kernelsize):\n",
    "        self.kernelsize = kernelsize\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=kernelsize, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=kernelsize, stride=1, padding=1)\n",
    "\n",
    "    def setStride(self, stride):\n",
    "        self.stride = stride\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=self.kernelsize, stride=stride, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=self.kernelsize, stride=stride, padding=1)\n",
    "\n",
    "    def setDropout(self, dropout):\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = CustomMultiMNISTDataset(\"../double_mnist_seed_123_image_size_64_64/train\", transform=transform)\n",
    "val_dataset = CustomMultiMNISTDataset(\"../double_mnist_seed_123_image_size_64_64/val\", transform=transform)\n",
    "test_dataset = CustomMultiMNISTDataset(\"../double_mnist_seed_123_image_size_64_64/test\", transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of validation samples:\", len(val_dataset))\n",
    "print(\"Number of testing samples:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Validation Accuracy (Digit 1): 57.14%\n",
      "Epoch 1/10, Validation Accuracy (Digit 2): 63.86%\n",
      "Epoch 2/10, Validation Accuracy (Digit 1): 70.19%\n",
      "Epoch 2/10, Validation Accuracy (Digit 2): 71.58%\n",
      "Epoch 3/10, Validation Accuracy (Digit 1): 76.34%\n",
      "Epoch 3/10, Validation Accuracy (Digit 2): 78.15%\n",
      "Epoch 4/10, Validation Accuracy (Digit 1): 75.56%\n",
      "Epoch 4/10, Validation Accuracy (Digit 2): 79.76%\n",
      "Epoch 5/10, Validation Accuracy (Digit 1): 78.24%\n",
      "Epoch 5/10, Validation Accuracy (Digit 2): 78.94%\n",
      "Epoch 6/10, Validation Accuracy (Digit 1): 77.82%\n",
      "Epoch 6/10, Validation Accuracy (Digit 2): 80.75%\n",
      "Epoch 7/10, Validation Accuracy (Digit 1): 76.35%\n",
      "Epoch 7/10, Validation Accuracy (Digit 2): 79.74%\n",
      "Epoch 8/10, Validation Accuracy (Digit 1): 74.50%\n",
      "Epoch 8/10, Validation Accuracy (Digit 2): 81.19%\n",
      "Epoch 9/10, Validation Accuracy (Digit 1): 78.69%\n",
      "Epoch 9/10, Validation Accuracy (Digit 2): 81.86%\n",
      "Epoch 10/10, Validation Accuracy (Digit 1): 75.92%\n",
      "Epoch 10/10, Validation Accuracy (Digit 2): 80.35%\n"
     ]
    }
   ],
   "source": [
    "num_classes_per_digit = 10  \n",
    "total_num_classes = 2 * num_classes_per_digit\n",
    "model = CNNModel(total_num_classes)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0025)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        label1 = torch.zeros(len(labels[0]), num_classes_per_digit)\n",
    "        label2 = torch.zeros(len(labels[1]), num_classes_per_digit)\n",
    "        for i in range(len(labels[0])):\n",
    "            label1[i, labels[0][i]] = 1\n",
    "        for i in range(len(labels[1])):\n",
    "            label2[i, labels[1][i]] = 1\n",
    "        labels = torch.cat((label1, label2), dim=1)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct1 = 0\n",
    "        correct2 = 0\n",
    "        total = 0\n",
    "        for images, labels in val_loader:\n",
    "            label1 = torch.zeros(len(labels[0]), num_classes_per_digit)\n",
    "            label2 = torch.zeros(len(labels[1]), num_classes_per_digit)\n",
    "            for i in range(len(labels[0])):\n",
    "                label1[i, labels[0][i]] = 1\n",
    "            for i in range(len(labels[1])):\n",
    "                label2[i, labels[1][i]] = 1\n",
    "            labels = torch.cat((label1, label2), dim=1)\n",
    "            outputs = model(images)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct1 += (predicted[:, :num_classes_per_digit] == label1).all(1).sum().item()\n",
    "            correct2 += (predicted[:, num_classes_per_digit:] == label2).all(1).sum().item()\n",
    "\n",
    "        accuracy1 = 100 * correct1 / total\n",
    "        accuracy2 = 100 * correct2 / total\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Validation Accuracy (Digit 1): {accuracy1:.2f}%')\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Validation Accuracy (Digit 2): {accuracy2:.2f}%')\n",
    "        wandb.log({\"cnn_val_accuracy1\": accuracy1, \"cnn_val_accuracy2\": accuracy2, \"epoch\": epoch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For hyperparameters: (0.001, 3, 1, 0.2)\n",
      "Aggregate accuracy: 68.0\n",
      "For hyperparameters: (0.005, 5, 2, 0.5)\n",
      "Aggregate accuracy: 67.0\n",
      "For hyperparameters: (0.001, 5, 1, 0.5)\n",
      "Aggregate accuracy: 70.0\n",
      "For hyperparameters: (0.005, 3, 2, 0.2)\n",
      "Aggregate accuracy: 69.0\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning on 1/10th training data, 1/10th validation data and 1/10th test data\n",
    "\n",
    "learning_rates = [0.001, 0.005]\n",
    "kernel_sizes = [3, 5]\n",
    "stride_sizes = [1, 2]\n",
    "dropout_rates = [0.2, 0.5]\n",
    "num_classes_per_digit = 10\n",
    "total_num_classes = 2 * num_classes_per_digit\n",
    "\n",
    "results = []\n",
    "hyperparams_list = [(0.001, 3, 1, 0.2), (0.005, 5, 2, 0.5), (0.001, 5, 1, 0.5), (0.005, 3, 2, 0.2)]\n",
    "\n",
    "for hyperparams in hyperparams_list:\n",
    "    lr = hyperparams[0]\n",
    "    kernel_size = hyperparams[1]\n",
    "    stride = hyperparams[2]\n",
    "    dropout = hyperparams[3]\n",
    "    model = CNNModel(total_num_classes)\n",
    "    model.setKernelsize(kernel_size)\n",
    "    model.setStride(stride)\n",
    "    model.setDropout(dropout)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for images, labels in train_loader_hyper:\n",
    "            label1 = torch.zeros(len(labels[0]), num_classes_per_digit)\n",
    "            label2 = torch.zeros(len(labels[1]), num_classes_per_digit)\n",
    "            for i in range(len(labels[0])):\n",
    "                label1[i, labels[0][i]] = 1\n",
    "            for i in range(len(labels[1])):\n",
    "                label2[i, labels[1][i]] = 1\n",
    "            labels = torch.cat((label1, label2), dim=1)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct1 = 0\n",
    "            correct2 = 0\n",
    "            total = 0\n",
    "            for images, labels in val_loader_hyper:\n",
    "                label1 = torch.zeros(len(labels[0]), num_classes_per_digit)\n",
    "                label2 = torch.zeros(len(labels[1]), num_classes_per_digit)\n",
    "                for i in range(len(labels[0])):\n",
    "                    label1[i, labels[0][i]] = 1\n",
    "                for i in range(len(labels[1])):\n",
    "                    label2[i, labels[1][i]] = 1\n",
    "                labels = torch.cat((label1, label2), dim=1)\n",
    "                outputs = model(images)\n",
    "                predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                total += labels.size(0)\n",
    "                correct1 += (predicted[:, :num_classes_per_digit] == label1).all(1).sum().item()\n",
    "                correct2 += (predicted[:, num_classes_per_digit:] == label2).all(1).sum().item()\n",
    "\n",
    "            accuracy1 = 100 * correct1 / total\n",
    "            accuracy2 = 100 * correct2 / total\n",
    "            if epoch == num_epochs - 1:\n",
    "                aggregate_accuracy = (accuracy1 + accuracy2) // 2\n",
    "\n",
    "                print(\"For hyperparameters:\", hyperparams)\n",
    "                print(\"Aggregate accuracy:\", aggregate_accuracy)\n",
    "                results.append((lr, kernel_size, stride, dropout, accuracy1, accuracy2, aggregate_accuracy))\n",
    "                wandb.log({\"lr\": lr, \"kernel_size\": kernel_size, \"stride\": stride, \"dropout\": dropout, \"cnn_aggregate_accuracy\": aggregate_accuracy})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
